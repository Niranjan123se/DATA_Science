{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d7e4397d",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "Q2. What are the different methods used for Web Scraping?\n",
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "Q4. Why is flask used in this Web Scraping project?\n",
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "raw",
   "id": "99c73e16",
   "metadata": {},
   "source": [
    "Ans to q1 \n",
    "this is the process through which we can automate the featacing the information form the html pages. \n",
    "this is used in many ways including the data aggregation cleaninng  and analyzing.  eg business can scrap the compitetors prise.\n",
    "reasearch this is help to retrive the imfomation for the researc perpose.  \n",
    "moniter the treabds \n",
    "job finding, we can automate and extract the information form the online plateforms "
   ]
  },
  {
   "cell_type": "raw",
   "id": "22d60b4f",
   "metadata": {},
   "source": [
    "Ans to q 2 \n",
    "Manual Copy-Pasting: This method involves manually copying and pasting the desired data from websites into a local file or spreadsheet. It's a simple approach but time-consuming for large amounts of data.\n",
    "\n",
    "Text Pattern Matching: With this method, specific patterns or data are identified using text search techniques. Regular expressions (regex) are commonly used to define patterns and extract relevant information from the website's HTML source code.\n",
    "\n",
    "HTML Parsing: HTML parsing involves using libraries or tools to parse the HTML structure of web pages. Popular libraries like BeautifulSoup (Python) or Jsoup (Java) provide functions to navigate and extract data using CSS selectors or XPath expressions.\n",
    "\n",
    "Web Scraping Frameworks: Web scraping frameworks provide higher-level abstractions and utilities to facilitate web scraping. They handle tasks like making HTTP requests, handling cookies and sessions, and parsing HTML. Examples include Scrapy (Python), Puppeteer (JavaScript), and Selenium (multiple languages).\n",
    "\n",
    "API Scraping: Some websites offer APIs (Application Programming Interfaces) that allow structured access to their data. Instead of scraping HTML, developers can interact with the API directly to retrieve the desired information in a more structured format.\n",
    "\n",
    "Headless Browsers: Headless browsers simulate human browsing behavior and can interact with websites like regular browsers. Tools like Puppeteer or Selenium provide the ability to scrape websites that rely on JavaScript or require user interactions.\n",
    "\n",
    "Reverse Engineering APIs: This method involves inspecting network traffic and analyzing requests and responses using browser developer tools. By understanding how the website's API works, developers can make requests programmatically to retrieve data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef9b89c8",
   "metadata": {},
   "source": [
    "ans to q3\n",
    "the Beautiful soup is a popular lib used for the webscraping and parsing the HTML or XML documents. it proovides a convenient and intuitive interface to extract datta form hhtml or xml sourse code."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2afed6cf",
   "metadata": {},
   "source": [
    " q 4 )Why is flask used in this Web Scraping project?\n",
    " ans to Q4 FLask in Python based web freamwork often usedd inn webscraping projects for severral reasons \n",
    " 1. web interface it allows to creat the web interfrace or Api for web scraping projject. You can create custom routes and endpoints to receive requests, process scraping tasks, and return the scraped data.\n",
    " \n",
    " 2. this ca handel the get ND POST REQUEST \n",
    " 3.  this can intigrate with scraping libraries such as beautiful soup \n",
    " 4. Data presentation we can use css and other lannguage to modify the UI/UX\n",
    " 5.  we can depoly the flask app on clude easily \n",
    " 6. Authontication and authorization flask cam provide the the same servise \n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f438b414",
   "metadata": {},
   "source": [
    "Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "In the mentioned project, three AWS services have been utilized: AWS CodePipeline, Amazon EC2 (Elastic Compute Cloud), and AWS Elastic Beanstalk. Here's an explanation of each service and its purpose in the project:\n",
    "\n",
    "1. AWS CodePipeline:\n",
    "   - Use: AWS CodePipeline is a fully managed continuous delivery service that helps automate the software release process. It facilitates the building, testing, and deployment of the web scraping project code.\n",
    "   - Purpose: In the project, CodePipeline is used to orchestrate and automate the steps involved in the deployment process. It can be configured to automatically detect changes in the code repository and trigger the deployment pipeline, ensuring the latest version of the web scraping application is deployed.\n",
    "\n",
    "2. Amazon EC2 (Elastic Compute Cloud):\n",
    "   - Use: Amazon EC2 is a scalable cloud computing service that provides virtual servers (EC2 instances) for running applications in the cloud.\n",
    "   - Purpose: EC2 is used in the project to host the web scraping application. It provides virtual server instances that can be configured with the necessary software and resources to run the application. EC2 instances can be scaled up or down based on demand, ensuring the application can handle varying workloads efficiently.\n",
    "\n",
    "3. AWS Elastic Beanstalk:\n",
    "   - Use: AWS Elastic Beanstalk is a fully managed service that simplifies the deployment and management of applications in multiple languages. It automates infrastructure provisioning, load balancing, and capacity scaling.\n",
    "   - Purpose: Elastic Beanstalk is utilized in the project to simplify the deployment of the web scraping application. It abstracts the underlying infrastructure details and handles tasks such as application environment setup, load balancing configuration, and automatic scaling. It allows developers to focus more on the application code rather than managing the infrastructure.\n",
    "\n",
    "Overall, AWS CodePipeline is used to automate the deployment process, Amazon EC2 provides the hosting environment for the web scraping application, and AWS Elastic Beanstalk simplifies the deployment and management of the application in a scalable manner. These services collectively enable efficient and automated deployment and hosting of the web scraping project on AWS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
